{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KubeSaw","text":"<p>KubeSaw is a powerful solution designed to manage multi-tenant Kubernetes clusters, helping organizations minimize operational costs while maintaining high security standards. KubeSaw slices Kubernetes clusters into smaller, manageable units called Spaces, each composed of one or more Kubernetes Namespaces. While KubeSaw can be described as a Namespace-as-a-Service (NaaS) solution, it provides much more. KubeSaw also supports multi-cluster environments, allowing organizations to manage namespaces and users within their cluster fleet. KubeSaw empowers teams to manage one or more Kubernetes clusters at scale, ensuring security, flexibility, and cost efficiency.</p> <p>NOTE: KubeSaw supports only OpenShift clusters for now.</p>"},{"location":"#naas-via-spaces","title":"NaaS via Spaces","text":"<p>An essential part of each multi-tenant solution running in a Kubernetes cluster is namespace-as-a-service (NaaS) functionality. KubeSaw orchestrates namespaces through small, manageable units called Spaces. Each Space consists of one or more namespaces (and optionally cluster-scoped resources) that are provisioned and managed through a set of customizable templates. All namespaces provisioned as part of one Space share the same lifecycle and behave as one entity.</p>"},{"location":"#users","title":"Users","text":"<p>KubeSaw has an abstraction for a user entity. These entities store user information taken from the company\u2019s SSO and can be associated with Spaces. When there is a binding between a Space and user, then KubeSaw grants corresponding permissions to the user in the underlying namespace(s). KubeSaw also provides and manages the full lifecycle of users and Spaces. Users can be in various states such as pending, approved, deactivated, or banned, with additional intermediate states.</p> <p>NOTE: KubeSaw supports only OpenID Connect compatible SSOs (eg. Keycloak) for now.</p>"},{"location":"#tiers","title":"Tiers","text":"<p>A Tier is a set of templates and additional configuration which define how Spaces and the underlying namespaces are provisioned in the cluster. The Tier templates can contain both namespace-scoped and cluster-scoped resources to be provisioned for each Space. By defining NetworkPolicies, limit ranges, quotas, and permissions, administrators can ensure that: Each Space - and its underlying namespaces - are provisioned securely and in isolation. Workloads are prevented from exceeding quotas, keeping infrastructure costs minimal.</p>"},{"location":"#multi-cluster","title":"Multi-cluster","text":"<p>To ensure sufficient scalability options, KubeSaw was designed to support multi-cluster environments. When KubeSaw is installed in a multi-cluster environment, then there is one cluster which works as the main control plane (called \u201chost\u201d), and one or more clusters (called \u201cmembers\u201d) where the actual Spaces - and the underlying namespaces - are provisioned. The host cluster can also function as a member cluster. It's common for teams to start with a single cluster. As its capacity is exceeded, additional member clusters can be added. This is true for KubeSaw environments, where the initial cluster can serve both as host and member, with more clusters added over time as needed. Multi-cluster architecture also helps with managing different types of clusters with different API versions, infrastructure or installed tools or operators.</p>"},{"location":"#proxy","title":"Proxy","text":"<p>In multi-cluster environments, it can be challenging for users to remember which clusters their Spaces are provisioned in. Users need to keep track of URLs and manage kubeconfigs for every cluster they have access to. To simplify this, KubeSaw offers a multi-cluster proxy. Users interact with a single URL, and only the context of the Space changes. The proxy forwards requests to the correct clusters based on the provided context. Users also use the company\u2019s SSO when authenticating against the proxy, without any need of getting the cluster-specific tokens.</p>"},{"location":"#ksctl","title":"Ksctl","text":"<p>For easy management of Spaces and users in the KubeSaw instance, there is a ksctl command-line tool that simplifies operations to one single command execution. By running ksctl commands, the administrators can approve, deactivate, or ban users, or for example promote a Space from one tier to another or re-provision the Space to a different member cluster.</p>"},{"location":"components/","title":"Components","text":"<p>This page provides a brief description of each KubeSaw component.</p>"},{"location":"components/#host-operator","title":"Host Operator","text":"<p>The host-operator is KubeSaw's control-plane, which is responsible for managing the users, spaces, notifications and monitoring the KubeSaw instance. One of the resources managed by host operator is the UserSignup resource, which is the source of truth for all user accounts. All the other user-related resources are created from the UserSignup.</p> <p>The source code is available here host-operator</p>"},{"location":"components/#member-operator","title":"Member Operator","text":"<p>The member operator is KubeSaw's data-plane, which is responsible for provisioning and managing the user namespaces and all the configurations inside those namespaces.</p> <p>There can be multiple member operators deployed as part of the same KubeSaw instance. Member operators are usually deployed per-cluster (member cluster) but can even co-exist in the same cluster.</p> <p>The source code is available here member-operator</p>"},{"location":"components/#registration-service","title":"Registration Service","text":"<p>The Registration Service is a standalone web application that runs on the same cluster as the host operator (host cluster). It provides a number of REST endpoints that the signup page can use to allow a user to sign up for a KubeSaw account.  The Registration Service application makes use of third party libraries such as Gin (an HTTP web framework) and Kubernetes client libraries (for interacting with the Kubernetes API) among others.</p>"},{"location":"components/#proxy","title":"Proxy","text":"<p>The Proxy is part of the registration service application deployment. It can be used in a multi-cluster environment to forward requests without the need to specify the cluster context, the proxy infers the target cluster from the user information and the target workspace/namespace of the request.</p> <p>Both Proxy and Registration-service are optional components and not required for having a fully functional KubeSaw instance. The registration service can be useful if your KubeSaw service requires a signup mechanism/frontend. The proxy can help, in the case of a multi-cluster solution, with removing some of the cluster awareness and friction for the end users or clients interacting with the member clusters. Another key feature of the proxy is that it accepts SSO tokens, not kube tokens. This keeps authorization and authentication centralized, which can be managed from the identity provider (atm only Keycloak is supported).</p> <p>The source code for both registration-service and proxy is available here registration-service</p>"},{"location":"components/#ksctl","title":"KSCTL","text":"<p>ksctl is a command-line tool that helps you manage your KubeSaw instance. It provides commands for managing the clusters, spaces and users in KubeSaw.</p> <p>The source code is available here ksctl</p> <p>See also the ksctl cheat sheet</p>"},{"location":"contributing/","title":"Contributing","text":"<p>This document describes how to install KubeSaw in a development environment.</p>"},{"location":"contributing/#prerequisites","title":"Prerequisites","text":""},{"location":"contributing/#openshift-cluster","title":"OpenShift Cluster","text":"<p>Ensure you have access to an OpenShift 4.6+ cluster with cluster admin privileges and log in using <code>oc login</code></p>"},{"location":"contributing/#required-tools","title":"Required Tools","text":"<p>Install the required tools.</p>"},{"location":"contributing/#authentication","title":"Authentication","text":"<p>Configure authentication for the cluster using one of the following options:</p> <ol> <li> <p>Configure your own Keycloak server and set up authentication on the OpenShift cluster: configuring internal oauth</p> </li> <li> <p>Deploy and configure keycloak internally as part of the cluster. Just add <code>DEV_SSO=true</code> parameter to the <code>dev</code> targets. For eg.: <code>make dev-deploy-latest DEV_SSO=true</code> will deploy latest version of the operators with a preconfigured keycloak instance and one default keycloak user <code>user1@user.us</code> with password <code>user1</code>. If you are presented with the following error, then you need to accept the self-signed certificate of the dev Keycloak instance first. Go to `https://keycloak-./auth (the complete link is printed out at the end of the command) and accept the certificate.: <p></p> <p>NOTE: This option only works with OCP and CRC clusters atm.</p> <ol> <li>If you're a RedHatter, contact a member of the KubeSaw Team for instructions on how to configure the cluster to use our internal Dev SSO.</li> </ol>"},{"location":"contributing/#install","title":"Install","text":""},{"location":"contributing/#remove-self-provisioner-role","title":"Remove Self Provisioner Role","text":"<p>It is strongly recommended to remove the self-provisioner role to disallow users from creating their own namespaces. This is because KubeSaw is designed to create/manage namespaces for users automatically. It creates these namespaces based on predefined templates that also define resource limits so only these namespaces should be accessible to KubeSaw users.</p> <p>Run the following commands:</p> <pre><code>oc patch clusterrolebinding.rbac self-provisioners -p '{\"subjects\": null, \"metadata\": {\"annotations\":{\"rbac.authorization.kubernetes.io/autoupdate\": \"false\"}}}'\noc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth\n</code></pre>"},{"location":"contributing/#install-kubesaw","title":"Install KubeSaw","text":"<p>Clone this repository <code>git clone git@github.com:codeready-toolchain/toolchain-e2e.git</code></p> <p>This repository provides you multiple Makefile targets that you can use - it depends on which version of KubeSaw operators you want to install.</p> <p>NOTE: If the cluster is an OSD cluster, then set the variable <code>IS_OSD=true</code> when running any of the Makefile targets (for example: <code>make dev-deploy-latest IS_OSD=true</code>).</p> <p>IMPORTANT: Make note of the Registration Service URL that is printed at the end of the target execution.</p>"},{"location":"contributing/#latest-greatest-kubesaw","title":"Latest greatest KubeSaw","text":"<p>Run the following to install the latest greatest KubeSaw operators in dev mode:</p> <pre><code>make dev-deploy-latest\n</code></pre>"},{"location":"contributing/#latest-greatest-kubesaw-for-appstudio","title":"Latest greatest KubeSaw for AppStudio","text":"<p>Run the following to install the latest greatest KubeSaw operators in dev mode for AppStudio environment:</p> <pre><code>make appstudio-dev-deploy-latest\n</code></pre>"},{"location":"contributing/#local-version","title":"Local version","text":"<p>If you want to install a local version of a specific KubeSaw operator in dev mode then:</p> <ol> <li>Configure your quay account for dev deployment</li> <li>Clone the <code>toolchain-e2e</code> repo (<code>git clone git@github.com:codeready-toolchain/toolchain-e2e.git</code>) and the operator(s) repo(s) that you'd like to test locally. They have to share the same parent folder for the local deployment to work, eg:</li> </ol> <pre><code>$ tree -d -L 1\n.\n\u251c\u2500\u2500 host-operator\n\u251c\u2500\u2500 member-operator\n\u251c\u2500\u2500 registration-service\n\u251c\u2500\u2500 toolchain-e2e\n</code></pre> <ol> <li>From the <code>toolchain-e2e</code> repo folder, run any of the following commands:</li> </ol> <pre><code># To deploy local versions of all repositories:\nmake dev-deploy-e2e-local\n\n# To deploy local version only of the host-operator repo:\nmake dev-deploy-e2e-host-local\n\n# To deploy local version only of the member-operator repo:\nmake dev-deploy-e2e-member-local\n\n# To deploy local version only of the registration-service repo:\nmake dev-deploy-e2e-registration-local\n</code></pre>"},{"location":"contributing/#toolchainstatus","title":"ToolchainStatus","text":"<p>Run the following command:</p> <pre><code>$ oc get toolchainstatus -n toolchain-host-operator\n\nNAME               MURS   READY   LAST UPDATED\ntoolchain-status   0      True    2021-03-24T22:39:36Z\n</code></pre> <p>and ensure the Ready status is <code>True</code></p>"},{"location":"contributing/#register-a-new-kubesaw-user","title":"Register a new KubeSaw User","text":"<ol> <li> <p>Open the Registration Service URL in a browser and sign up for an account.</p> </li> <li> <p>Wait for the message \"Your OpenShift Developer Sandbox account is waiting for approval\"</p> </li> </ol> <p></p>"},{"location":"contributing/#manual-approval","title":"Manual Approval","text":"<p>Manual approval means each UserSignup must be approved by editing the UserSignup resource for a particular user.</p> <ol> <li>Run the following command to get the name of the UserSignup resource: <code>oc get usersignup</code>     The name should be a UUID eg. 66e54c45-9868-4a25-81ca-d56b600c8491</li> <li>Approve the UserSignup</li> </ol> <pre><code>oc patch usersignup -p '{\"spec\":{\"states\":[\"approved\"]}}' --type=merge &lt;usersignup name&gt; -n &lt;host operator namespace&gt;\n</code></pre>"},{"location":"contributing/#automatic-approval","title":"Automatic Approval","text":"<p>By enabling automatic approval in the KubeSaw configuration, users will be automatically approved and provisioned without admin intervention.</p> <p>Enable automatic approval by running the following command:</p> <pre><code>oc patch ToolchainConfig -p '{\"spec\":{\"host\":{\"automaticApproval\":{\"enabled\":true}}}}' --type=merge config -n &lt;host operator namespace&gt;\n</code></pre>"},{"location":"contributing/#using-kubesaw-instance","title":"Using KubeSaw instance","text":"<p>After approval, the registration service will display a link to start using the KubeSaw instance. The link will go to the user's Dev Console, but first, a login page will appear with two options.:</p> <ol> <li>kube:admin</li> <li>The authentication method configured in the Authentication step  Select option 2 and log in using the same account used from the Register a KubeSaw User section step.</li> </ol> <p>After logging in, the user will have access to only the namespaces created for them.</p>"},{"location":"contributing/#cleanup","title":"Cleanup","text":"<p>NOTE: the make targets are available in the <code>toolchain-e2e</code> repo (<code>git clone git@github.com:codeready-toolchain/toolchain-e2e.git</code>)</p>"},{"location":"contributing/#remove-only-users-and-their-namespaces","title":"Remove Only Users and Their Namespaces","text":"<p>Run <code>make clean-users</code></p>"},{"location":"contributing/#remove-all-sandbox-related-resources","title":"Remove All Sandbox-related Resources","text":"<p>Run <code>make clean-e2e-resources</code></p>"},{"location":"how-to-configure/","title":"How to configure","text":""},{"location":"how-to-configure/#create-toolchainconfiguration","title":"Create toolchainconfiguration","text":"<p>Api definition: toolchainconfig api</p> <p>In the host cluster create the <code>toolchainconfiguration</code> in <code>toolchain-host-operator</code> namespace:</p> <pre><code>apiVersion: toolchain.dev.openshift.com/v1alpha1\nkind: ToolchainConfig\nmetadata:\n  name: config\n  namespace: toolchain-host-operator\nspec:\n  host:\n    tiers:\n      defaultSpaceTier: 'base'\n    automaticApproval:\n      enabled: true\n    notifications:\n      adminEmail: &lt;admin-email-list&gt;@domain.com\n      secret:\n        mailgunAPIKey: mailgun.api.key\n        mailgunDomain: mailgun.domain\n        mailgunReplyToEmail: mailgun.replyto.email\n        mailgunSenderEmail: mailgun.sender.email\n        ref: host-operator-secret\n      templateSetName: 'templateSet name'\n    registrationService:\n      auth:\n        authClientConfigRaw: '{\n                  \"realm\": \"realm-name\",\n                  \"auth-server-url\": \"https://sso.domain.com/auth\",\n                  \"ssl-required\": \"ALL\",\n                  \"resource\": \"sso-resource-name\",\n                  \"clientId\": \"sso-client-id\",\n                  \"public-client\": true\n                }'\n        authClientLibraryURL: https://sso.domain.com/auth/js/keycloak.js\n        authClientPublicKeysURL: https://sso.domain.com/auth/realms/realm-name/protocol/openid-connect/certs\n        ssoBaseURL: https://sso.domain.com\n        ssoRealm: realm-name\n      environment: prod\n      replicas: 5\n      registrationServiceURL: https://&lt;my-domain&gt;\n  members:\n    default:\n      auth:\n        idp: IdpName\n      autoscaler:\n        bufferMemory: 3Gi\n        bufferReplicas: 10\n        deploy: true\n</code></pre>"},{"location":"how-to-configure/#members-configuration","title":"Members Configuration","text":"<p>Api definition: member operator config</p> <p>The <code>ToolchainConfig.Spec.Members</code> section is automatically propagated by the host operator to all member clusters. The host-operator will create a <code>MemberOperatorConfig</code> CR with the configuration specified in that section.</p> <p>You can also provide specific configuration per member cluster by configuring the <code>specificPerMemberCluster</code> field :</p> <pre><code>apiVersion: toolchain.dev.openshift.com/v1alpha1\nkind: ToolchainConfig\nmetadata:\n  name: config\n  namespace: toolchain-host-operator\nspec:\n   \u2026.\n   \u2026.\n  members:\n    default:\n      auth:\n        idp: DevSandbox\n      autoscaler: \n        bufferMemory: 3Gi\n        bufferReplicas: 10\n        deploy: true\n    specificPerMemberCluster:\n     &lt;memberClusterName&gt;:\n       webhook:\n         deploy: false\n</code></pre> <p>In the previous example, we are disabling the deployment of the webhook for a specific member cluster. The <code>specificPerMemberCluster</code> configuration will override the default configuration section for that member cluster.</p>"},{"location":"how-to-configure/#create-the-spaceprovisionerconfig","title":"Create the SpaceProvisionerConfig","text":"<p>Api definition: space provisioner config</p> <p>Create the <code>SpaceProvisionerConfig</code> in the <code>toolchain-host-operator</code> namespace which opens a member cluster for user/workload onboarding, there should be one <code>SpaceProvisionerConfig</code> for each registered member cluster:</p> <pre><code>apiVersion: toolchain.dev.openshift.com/v1alpha1\nkind: SpaceProvisionerConfig\nmetadata:\n  name: &lt;name&gt;\n  namespace: toolchain-host-operator\nspec:\n  toolchainCluster: &lt;name of the toolchaincluster CR in toolchain-host-operator namespace&gt;\n  enabled: true\n  capacityThresholds:\n    maxNumberOfSpaces: 1000\n    maxMemoryUtilizationPercent: 90\n  placementRoles:\n  - cluster-role.toolchain.dev.openshift.com/tenant\n</code></pre>"},{"location":"how-to-configure/#create-secrets-referenced-in-toolchainconfig","title":"Create secrets referenced in toolchainconfig","text":"<p>Create the <code>host-operator-secret</code> in the <code>toolchain-host-operator</code> namespace:</p> <pre><code>kind: Secret\napiVersion: v1\nmetadata:\n  name: host-operator-secret\n  namespace: toolchain-host-operator\ndata:\n  mailgun.api.key: &lt;base64 api key&gt;\n  mailgun.domain: &lt;base64 mailgun domain&gt;\n  mailgun.replyto.email: &lt;base64 reply to email&gt;\n  mailgun.sender.email: &lt;base64 sender email&gt;\ntype: Opaque\n</code></pre> <p>Create the <code>member-operator-secret</code> in <code>toolchain-member-operator</code> namespace:</p> <pre><code>kind: Secret\napiVersion: v1\nmetadata:\n  name: member-operator-secret\n  namespace: toolchain-member-operator\ndata:\n  github.access.token: &lt;base64 token&gt;  \ntype: Opaque\n</code></pre>"},{"location":"how-to-configure/#secret-loading","title":"Secret loading","text":"<p>As you can notice, the name of the secret is referenced in the <code>toolchainconfig</code> CR ( see <code>ToolchainConfig.Spec.Host.Notifications.Secret.Ref</code> field ) and every field in the data section of the secret was referenced also in the <code>ToolchainConfig.Spec.Host.Notifications.Secret</code>, in this way the host-operator would know which secret and which fields to load the configuration from. Same thing for secrets referenced in the members section, which will be read by the member operator.</p>"},{"location":"how-to-configure/#provisioning-kubesaw-admins","title":"Provisioning KubeSaw admins","text":"<p>You can set up admin access to the KubeSaw instances by leveraging the ksctl command line, please refer to the Admin Usage section. </p>"},{"location":"how-to-install/","title":"How to install","text":""},{"location":"how-to-install/#prerequisites","title":"Prerequisites","text":""},{"location":"how-to-install/#cluster-requirements","title":"Cluster requirements","text":"<p>TODO </p> <p>Add here cluster requirements ( eg. 1 host cluster with N nodes , 2 member clusters with N nodes each .. ) and cluster configuration instructions</p>"},{"location":"how-to-install/#software-requirements","title":"Software requirements","text":"<p>Openshift cluster - ( tested with OCP, OSD, Rosa and CRC )</p>"},{"location":"how-to-install/#install-host-operator","title":"Install Host Operator","text":"<p>Use ksctl CLI to install the host operator:</p> <pre><code>ksctl adm install-operator host --kubeconfig &lt;path/to/host/kubeconfig&gt; --namespace toolchain-host-operator\n</code></pre> <p>The command will create the namespace (if it doesn\u2019t exist) and deploy the CatalogSource, OperatorGroup and Subscriptions for the latest version of the host operator</p>"},{"location":"how-to-install/#install-member-operator","title":"Install Member Operator","text":"<p>Use ksctl CLI to install the member operator:</p> <pre><code>ksctl adm install-operator member --kubeconfig &lt;path/to/member/kubeconfig&gt; --namespace &lt;namespace&gt;\n</code></pre> <p>The command will create the namespace (if it doesn\u2019t exist) and deploy the CatalogSource, OperatorGroup and Subscriptions for the latest version of the member operator.</p> <p>You can repeat this command for all the member clusters you have on which you want to deploy the member operator.</p>"},{"location":"how-to-install/#register-member-clusters","title":"Register member clusters","text":"<p>Use ksctl CLI to register a member cluster in the host operator:</p> <pre><code>ksctl adm register-member --host-kubeconfig &lt;host/kubeconfig&gt; --member-kubeconfig &lt;member/kubeconfig&gt;\n</code></pre> <p>Note: if the cluster uses let\u2019s encrypt certificates the \u2013lets-encrpyt flag should be provided to the above command</p>"},{"location":"ksctl-cheat-sheet/","title":"Cheat sheet for commands","text":"<p>This section covers the most useful commands. However, the <code>ksctl</code> binary provides other commands than those that are listed here. To see all of them, run:</p> <pre><code>ksctl --help\n</code></pre> <p>The <code>ksctl --version</code> command shows the version of the binary (based on the commit hash)</p> <pre><code>ksctl --version\n</code></pre> <p>NOTE: Prerequisite: The <code>.ksctl.yaml</code> config file is needed to run user-management related <code>ksctl</code> commands. The default location is your home directory: <code>~/.ksctl.yaml</code>, but you can use the <code>--config</code> flag to specify a different path. It contains the configuration settings for the host and member clusters together with the granted token.</p>"},{"location":"ksctl-cheat-sheet/#finding-usersignup-name","title":"Finding UserSignup name","text":"<p>When users sign up, a <code>UserSignup</code> resource is created on their behalf on the Host cluster. For most of the user-management operations, the name of the <code>UserSignup</code> resource is needed. To see all <code>UserSignup</code> resource names run:</p> <pre><code>$ ksctl get usersignup -t host\n\nNAME                 USERNAME     COMPLETE   REASON            COMPLIANTUSERNAME   EMAIL\n...\n2237e8be-f678d76ff   dummy-name   False      PendingApproval                       dummy@email.com\n...\n</code></pre> <p>The first column is the name of the <code>UserSignup</code> resource.</p> <p>To look up a UserSignup resource from the user's email address, run: in Linux:</p> <pre><code>ksctl get -t host usersignups -l toolchain.dev.openshift.com/email-hash=`echo -n &lt;email_address&gt; | md5sum | cut -d ' ' -f 1`\n</code></pre> <p>in macOS:</p> <pre><code>ksctl get -t host usersignups -l toolchain.dev.openshift.com/email-hash=`echo -n &lt;email_address&gt; | md5`\n</code></pre>"},{"location":"ksctl-cheat-sheet/#approving-a-user","title":"Approving a user","text":"<p>To approve a user, either use the user's email:</p> <pre><code>ksctl approve --email &lt;user_email&gt;\n</code></pre> <p>or get the UserSignup name, and then run:</p> <pre><code>ksctl approve --name &lt;usersignup_name&gt;\n</code></pre> <p>WARNING: By default, the <code>approve</code> command checks if the user has already initiated the phone verification process. To skip this check for the users or environments where the phone verification is not required, use the <code>--skip-phone-check</code> flag.</p> <p>The command will print out additional information about the <code>UserSignup</code> resource to be approved, and it will also ask for a confirmation.</p>"},{"location":"ksctl-cheat-sheet/#deactivating-a-user","title":"Deactivating a user","text":"<p>To deprovision a user from the platform and keep the <code>UserSignup</code> resource there, use <code>deactivate</code> command. First get the UserSignup name, then run:</p> <pre><code>ksctl deactivate &lt;usersignup_name&gt;\n</code></pre> <p>The command will print out additional information about the <code>UserSignup</code> resource to be deactivated and it will also ask for a confirmation.</p>"},{"location":"ksctl-cheat-sheet/#deleting-a-user","title":"Deleting a user","text":"<p>To completely remove a user from the platform including the <code>UserSignup</code> resource (for example, as part of a GDPR request), use the <code>gdpr-delete</code> command. First get the UserSignup name, then run:</p> <pre><code>ksctl gdpr-delete &lt;usersignup_name&gt;\n</code></pre> <p>The command will print out additional information about the <code>UserSignup</code> resource to be deleted and it will also ask for a confirmation.</p>"},{"location":"ksctl-cheat-sheet/#banning-a-user","title":"Banning a user","text":"<p>To ban a user which in turn de-provisions the account and doesn't allow the user to sign up again, use the <code>ban</code> command. First get the UserSignup name, then run:</p> <pre><code>ksctl ban &lt;usersignup_name&gt; &lt;ban_reason&gt;\n</code></pre> <p>The command will print out additional information about the <code>UserSignup</code> resource to be banned, and it will also ask for a confirmation.</p>"},{"location":"ksctl-cheat-sheet/#creating-an-event","title":"Creating an Event","text":"<p>Social Events are a feature allowing users to sign up without having to go through the phone verification process. This is useful when running labs or workshops, as it lets attendees get up and run it quickly without having to fulfill all the requirements of the standard sign up process.</p> <p>Social Events are temporary in nature; creating an event will produce a unique activation code that may be used for a predefined period of time, after which the code will no longer work.</p> <p>Use the <code>create-event</code> command to create a new event, specifying a <code>description</code>, the <code>start-date</code> and <code>end-date</code> range and <code>max-attendees</code>.  The date range should encompass the dates of the event (it is recommended that the range actually be expanded to include the day before and after the event just to be safe), and the maximum attendees should also be slightly higher than the expected attendees in the rare case of technical difficulties or additional attendees.</p> <p>Here's an actual example:</p> <pre><code>ksctl create-event --description=\"Summit Connect Dallas / SF\" --start-date=2022-09-27 --end-date=2022-09-30 --max-attendees=70\n</code></pre> <p>The output from this command should look something like this:</p> <pre><code>Social Event successfully created. Activation code is 'bduut'\n</code></pre> <p>The activation code should be kept secret, and only provided to the event organizer.</p>"},{"location":"ksctl-cheat-sheet/#admin-usage","title":"Admin usage","text":"<p>There is a provisioning flow for KubeSaw administrators separate from what the standard KubeSaw users use when they are signing up through the registration service. There are two ways of granting permissions to the KubeSaw administrators, either via a ServiceAccount or via an OpenShift user.</p>"},{"location":"ksctl-cheat-sheet/#admin-manifests","title":"Admin manifests","text":"<p>The admin manifests are generated via <code>ksctl generate admin-manifests</code> command. The command generates manifests in a Kustomize folders, so it can be easily synced by another tool (eg. ArgoCD) to the cluster. The content of the admin manifests is defined in <code>kubesaw-admins.yaml</code> file, which is used also as the source for <code>ksctl generate admin-manifests</code> command. You can see an example of such a file in kubesaw-admins.yaml.</p>"},{"location":"ksctl-cheat-sheet/#clusters","title":"Clusters","text":"<p>The required sections of the <code>kubesaw-admins.yaml</code> file is a <code>clusters</code> section defining location and names of the clusters used in the KubeSaw instance. This is necessary for running <code>ksctl generate cli-configs</code> command which adds the information to all generated <code>ksctl.yaml</code> files.</p> <pre><code>clusters:\n  host:\n    api: https://api.dummy-host.openshiftapps.com:6443\n  members:\n  - api: https://api.dummy-m1.openshiftapps.com:6443\n    name: member-1\n  - api: https://api.dummy-m2.openshiftapps.com:6443\n    name: member-2\n</code></pre>"},{"location":"ksctl-cheat-sheet/#add-serviceaccount-for-cli-usage","title":"Add ServiceAccount for cli usage","text":"<p>The <code>serviceAccounts</code> section contains definition of ServiceAccounts together with the granted permissions. To add a new SA that is supposed to be used in a combination with cli commands, add the following code:</p> <pre><code>serviceAccounts:\n- name: &lt;your-name&gt;\n  host:\n    roleBindings:\n    - namespace: toolchain-host-operator\n      roles:\n      - &lt;roles-or-commands-to-be-granted&gt;\n    clusterRoleBindings:\n      clusterRoles:\n      - ...\n\n  member:\n    roleBindings:\n    - namespace: toolchain-member-operator\n      roles:\n      - &lt;roles-or-commands-to-be-granted&gt;\n    clusterRoleBindings:\n      clusterRoles:\n      - ...\n</code></pre>"},{"location":"ksctl-cheat-sheet/#serviceaccount-namespace-location","title":"ServiceAccount namespace location","text":"<p>By default, all <code>ServiceAccounts</code> are created in default namespaces:</p> <ul> <li><code>kubesaw-admins-host</code> for the host cluster</li> <li><code>kubesaw-admins-meber</code> for the member cluster</li> </ul> <p>The default location can be changed in <code>kubesaw-admin.yaml</code> file:</p> <pre><code>defaultServiceAccountsNamespace:\n  host: your-host-namespace\n  member: your-member-namespace\n</code></pre> <p>These two namespaces have to be named differently.</p> <p>It's also possible to override the namespace location for a given <code>ServiceAccount</code>:</p> <pre><code>serviceAccounts:\n- name: in-namespace-sa\n  namespace: specific-sa-namespace\n  host:\n    ...\n  member:\n    ...\n</code></pre>"},{"location":"ksctl-cheat-sheet/#generate-ksctlyaml-files","title":"Generate ksctl.yaml files","text":"<p>For each ServiceAccount defined in this section, the <code>ksctl generate cli-configs</code> generates a separate <code>ksctl.yaml</code> file with the corresponding cluster configuration and tokens. As an administrator of the clusters, run this command and distribute securely the generated <code>ksctl.yaml</code> files to other team members.</p>"},{"location":"ksctl-cheat-sheet/#testing-the-ksctl-generate-cli-configs-command-locally","title":"Testing the <code>ksctl generate cli-configs</code> command locally","text":"<ol> <li>Run <code>make install</code></li> <li>Create <code>kubesaw-admins.yaml</code> (as an example, check kubesaw-admins.yaml.)</li> <li>Run <code>ksctl generate admin-manifests --kubesaw-admins &lt;path&gt;/kubesaw-admins.yaml --out-dir &lt;admin-manifests-out-dir-path&gt;</code></li> <li>Create resources from the <code>&lt;admin-manifests-out-dir-path&gt;</code> of the previous command. Please, note that you will need to create some namespaces manually (<code>oc create ns &lt;namespace-name&gt;</code>), such as <code>host-sre-namespace</code>, <code>first-component</code>, <code>second-component</code>, <code>some-component</code>, <code>member-sre-namespace</code>, and <code>crw</code>, for example.</li> <li>Run <code>oc apply -k &lt;admin-manifests-out-dir-path&gt;/host</code></li> <li>Run <code>oc apply -k &lt;admin-manifests-out-dir-path&gt;/member</code></li> <li>Run <code>oc apply -k &lt;admin-manifests-out-dir-path&gt;/member-3</code></li> <li>Run <code>ksctl generate cli-configs -k &lt;kubeconfig-path&gt; -c &lt;path&gt;/kubesaw-admins.yaml</code></li> </ol>"},{"location":"ksctl-cheat-sheet/#users","title":"Users","text":"<p>The <code>ksctl</code> command can generate  The <code>users</code> section contains definition for users, identities, and the permissions granted to them. KubeSaw uses a suffix <code>-crtadmin</code> for the admin usernames which are blocked from signing-up as a regular users via registration service. This ensures that provisioning admin users is fully isolated from the process of the regular ones. To add a -crtadmin user for a particular component in member cluster, update the corresponding <code>kubesaw-admins.yaml</code> file by adding the following code under the <code>users</code> section:</p> <p>For an admin of the component that needs to manually approve operator updates:</p> <pre><code>users:\n- name: &lt;your-name&gt;-maintainer\n  id:\n  - &lt;sso-identities&gt;\n  member:\n    roleBindings:\n    - namespace: &lt;namespace-name&gt;\n      roles:\n      - view-secrets\n      clusterRoles:\n      - &lt;edit/admin&gt;\n      - some-extra-permissions\n    clusterRoleBindings:\n      clusterRoles:\n      - some-extra-cluster-scope-permissions\n</code></pre> <p>NOTE: The creation of the ClusterRoles is not managed via ksctl, you need to make sure that they are created in the cluster.</p> <p>For a maintainer of the component with limited permissions:</p> <pre><code>- name: &lt;your-name&gt;-maintainer\n  id:\n  - &lt;sso-identities&gt;\n  member:\n    roleBindings:\n    - namespace: &lt;namespace-name&gt;\n      clusterRoles:\n      - &lt;edit/view&gt;\n</code></pre> <p>If you need any permissions also in a namespace in host cluster (to be used mainly by KubeSaw maintainers), then include the host section in the user's definition as well:</p> <pre><code>- name: &lt;your-name&gt;-maintainer\n  id:\n  - &lt;sso-identities&gt;\n  host:\n    roleBindings:\n    - namespace: &lt;namespace-name&gt;\n    ...\n  member:\n    roleBindings:\n    - namespace: &lt;namespace-name&gt;\n    ...\n</code></pre>"},{"location":"quay-repo-config/","title":"Quay repo config","text":""},{"location":"quay-repo-config/#configure-your-quay-account-for-dev-deployment","title":"Configure your Quay account for dev deployment","text":"<p>There is a set of images that are built and pushed to quay repositories while deploying local versions of KubeSaw operators to OpenShift cluster. Please make sure that the repositories exist in your quay.io account.</p>"},{"location":"quay-repo-config/#repositories","title":"Repositories","text":"<ol> <li>Register for a quay.io account if you don't have one</li> <li>Make sure you have set the QUAY_NAMESPACE variable: <code>export QUAY_NAMESPACE=&lt;quay-username&gt;</code></li> <li>Log in to quay.io using +    <code>docker login quay.io</code></li> <li>Make sure that these repositories exist on quay.io and the visibility is set to <code>public</code> for all of them:<ul> <li><code>https://quay.io/repository/&lt;quay-username&gt;/host-operator</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/host-operator-bundle</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/host-operator-index</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/member-operator</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/member-operator-webhook</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/member-operator-bundle</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/member-operator-index</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/registration-service</code></li> </ul> </li> </ol>"},{"location":"quay-repo-config/#public-visibility","title":"Public visibility","text":"<p>All aforementioned repositories have to be public, so make sure that the visibility is set to <code>public</code> for all of them:</p> <ul> <li><code>https://quay.io/repository/&lt;quay-username&gt;/host-operator?tab=settings</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/host-operator-bundle?tab=settings</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/host-operator-index?tab=settings</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/member-operator?tab=settings</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/member-operator-webhook?tab=settings</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/member-operator-bundle?tab=settings</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/member-operator-index?tab=settings</code></li> <li><code>https://quay.io/repository/&lt;quay-username&gt;/registration-service?tab=settings</code></li> </ul>"},{"location":"required-tools/","title":"Required tools","text":""},{"location":"required-tools/#required-pre-installed-tools","title":"Required Pre-installed Tools","text":"<ul> <li>go 1.20.x (1.20.11 or higher)</li> <li>git</li> <li>operator-sdk 1.25.0 +   NOTE: Follow the installation instructions here. Make sure that the download URL (specified by the <code>OPERATOR_SDK_DL_URL</code> environment variable) is set to the correct version.</li> <li>sed</li> <li>yamllint</li> <li>jq</li> <li>podman +   NOTE: If you need to use docker, then run the make targets with this variable set: <code>IMAGE_BUILDER=docker</code>.</li> <li>opm v1.26.3 +   NOTE: To download the Operator Registry tool use either https://github.com/operator-framework/operator-registry/releases or https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/. The version should correspond with the OpenShift version you are running. To confirm that the Operator Registry tool is installed correctly: <code>$ opm version</code></li> </ul>"}]}